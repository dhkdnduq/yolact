{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.augmentations import FastBaseTransform\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import onnxruntime as rt\n",
    "import time\n",
    "\n",
    "# Load model\n",
    "sess = rt.InferenceSession(\"yolact.onnx\")\n",
    "input_name = sess.get_inputs()[0].name\n",
    "loc_name = sess.get_outputs()[0].name\n",
    "conf_name = sess.get_outputs()[1].name\n",
    "mask_name = sess.get_outputs()[2].name\n",
    "priors_name = sess.get_outputs()[3].name\n",
    "proto_name = sess.get_outputs()[4].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_path = \"/media/ernestas/D1/workdir/yolact/data/city.jpg\"\n",
    "frame = torch.from_numpy(cv2.imread(input_file_path)).float()\n",
    "batch = FastBaseTransform()(frame.unsqueeze(0))\n",
    "# inference\n",
    "pred_onx = sess.run([loc_name, conf_name, mask_name, priors_name, proto_name], {input_name: batch.cpu().detach().numpy()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average inference time: 0.25615954399108887\n",
      "Average inference time: 0.2530597448348999\n",
      "Average inference time: 0.251775582631429\n",
      "Average inference time: 0.2516060471534729\n",
      "Average inference time: 0.2544114589691162\n",
      "Average inference time: 0.25374384721120197\n",
      "Average inference time: 0.25311626706804546\n",
      "Average inference time: 0.25288474559783936\n",
      "Average inference time: 0.25280822647942436\n",
      "Average inference time: 0.25245592594146726\n",
      "Average inference time: 0.2527274868705056\n",
      "Average inference time: 0.2526486913363139\n",
      "Average inference time: 0.25239298893855167\n",
      "Average inference time: 0.25249227455684115\n",
      "Average inference time: 0.25254376729329425\n",
      "Average inference time: 0.2588084042072296\n",
      "Average inference time: 0.25841675085179944\n",
      "Average inference time: 0.25787799888186985\n",
      "Average inference time: 0.25750874218187836\n",
      "Average inference time: 0.2572284936904907\n",
      "Average inference time: 0.2608751228877476\n",
      "Average inference time: 0.2616101395000111\n",
      "Average inference time: 0.2627324436021888\n",
      "Average inference time: 0.2622712353865306\n",
      "Average inference time: 0.262262716293335\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "for i in range(25):\n",
    "    pred_onx = sess.run([loc_name, conf_name, mask_name, priors_name, proto_name], {input_name: batch.cpu().detach().numpy()})\n",
    "    print(f\"Average inference time: {(time.time()-start)/(i+1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(det_output, w, h, batch_idx=0, interpolation_mode='bilinear',\n",
    "                visualize_lincomb=False, crop_masks=True, score_threshold=0):\n",
    "    \"\"\"\n",
    "    Postprocesses the output of Yolact on testing mode into a format that makes sense,\n",
    "    accounting for all the possible configuration settings.\n",
    "\n",
    "    Args:\n",
    "        - det_output: The lost of dicts that Detect outputs.\n",
    "        - w: The real with of the image.\n",
    "        - h: The real height of the image.\n",
    "        - batch_idx: If you have multiple images for this batch, the image's index in the batch.\n",
    "        - interpolation_mode: Can be 'nearest' | 'area' | 'bilinear' (see torch.nn.functional.interpolate)\n",
    "\n",
    "    Returns 4 torch Tensors (in the following order):\n",
    "        - classes [num_det]: The class idx for each detection.\n",
    "        - scores  [num_det]: The confidence score for each detection.\n",
    "        - boxes   [num_det, 4]: The bounding box for each detection in absolute point form.\n",
    "        - masks   [num_det, h, w]: Full image masks for each detection.\n",
    "    \"\"\"\n",
    "    \n",
    "    dets = det_output[batch_idx]\n",
    "    net = dets['net']\n",
    "    dets = dets['detection']\n",
    "\n",
    "    if dets is None:\n",
    "        return [torch.Tensor()] * 4 # Warning, this is 4 copies of the same thing\n",
    "\n",
    "    if score_threshold > 0:\n",
    "        keep = dets['score'] > score_threshold\n",
    "\n",
    "        for k in dets:\n",
    "            if k != 'proto':\n",
    "                dets[k] = dets[k][keep]\n",
    "        \n",
    "        if dets['score'].size(0) == 0:\n",
    "            return [torch.Tensor()] * 4\n",
    "    \n",
    "    # Actually extract everything from dets now\n",
    "    classes = dets['class']\n",
    "    boxes   = dets['box']\n",
    "    scores  = dets['score']\n",
    "    masks   = dets['mask']\n",
    "\n",
    "    if cfg.mask_type == mask_type.lincomb and cfg.eval_mask_branch:\n",
    "        # At this points masks is only the coefficients\n",
    "        proto_data = dets['proto']\n",
    "        \n",
    "        # Test flag, do not upvote\n",
    "        if cfg.mask_proto_debug:\n",
    "            np.save('scripts/proto.npy', proto_data.cpu().numpy())\n",
    "        \n",
    "        if visualize_lincomb:\n",
    "            display_lincomb(proto_data, masks)\n",
    "\n",
    "        masks = proto_data @ masks.t()\n",
    "        masks = cfg.mask_proto_mask_activation(masks)\n",
    "\n",
    "        # Crop masks before upsampling because you know why\n",
    "        if crop_masks:\n",
    "            masks = crop(masks, boxes)\n",
    "\n",
    "        # Permute into the correct output shape [num_dets, proto_h, proto_w]\n",
    "        masks = masks.permute(2, 0, 1).contiguous()\n",
    "\n",
    "        if cfg.use_maskiou:\n",
    "            with timer.env('maskiou_net'):                \n",
    "                with torch.no_grad():\n",
    "                    maskiou_p = net.maskiou_net(masks.unsqueeze(1))\n",
    "                    maskiou_p = torch.gather(maskiou_p, dim=1, index=classes.unsqueeze(1)).squeeze(1)\n",
    "                    if cfg.rescore_mask:\n",
    "                        if cfg.rescore_bbox:\n",
    "                            scores = scores * maskiou_p\n",
    "                        else:\n",
    "                            scores = [scores, scores * maskiou_p]\n",
    "\n",
    "        # Scale masks up to the full image\n",
    "        masks = F.interpolate(masks.unsqueeze(0), (h, w), mode=interpolation_mode, align_corners=False).squeeze(0)\n",
    "\n",
    "        # Binarize the masks\n",
    "        masks.gt_(0.5)\n",
    "\n",
    "    \n",
    "    boxes[:, 0], boxes[:, 2] = sanitize_coordinates(boxes[:, 0], boxes[:, 2], w, cast=False)\n",
    "    boxes[:, 1], boxes[:, 3] = sanitize_coordinates(boxes[:, 1], boxes[:, 3], h, cast=False)\n",
    "    boxes = boxes.long()\n",
    "\n",
    "    if cfg.mask_type == mask_type.direct and cfg.eval_mask_branch:\n",
    "        # Upscale masks\n",
    "        full_masks = torch.zeros(masks.size(0), h, w)\n",
    "\n",
    "        for jdx in range(masks.size(0)):\n",
    "            x1, y1, x2, y2 = boxes[jdx, :]\n",
    "\n",
    "            mask_w = x2 - x1\n",
    "            mask_h = y2 - y1\n",
    "\n",
    "            # Just in case\n",
    "            if mask_w * mask_h <= 0 or mask_w < 0:\n",
    "                continue\n",
    "            \n",
    "            mask = masks[jdx, :].view(1, 1, cfg.mask_size, cfg.mask_size)\n",
    "            mask = F.interpolate(mask, (mask_h, mask_w), mode=interpolation_mode, align_corners=False)\n",
    "            mask = mask.gt(0.5).float()\n",
    "            full_masks[jdx, y1:y2, x1:x2] = mask\n",
    "        \n",
    "        masks = full_masks\n",
    "\n",
    "    return classes, scores, boxes, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_display(dets_out, img, h, w, undo_transform=True, class_color=False, mask_alpha=0.45):\n",
    "    \"\"\"\n",
    "    Note: If undo_transform=False then im_h and im_w are allowed to be None.\n",
    "    \"\"\"\n",
    "    if undo_transform:\n",
    "        img_numpy = undo_image_transformation(img, w, h)\n",
    "        img_gpu = torch.Tensor(img_numpy)\n",
    "    else:\n",
    "        img_gpu = img / 255.0\n",
    "        h, w, _ = img.shape\n",
    "    \n",
    "    with timer.env('Postprocess'):\n",
    "        t = postprocess(dets_out, w, h, visualize_lincomb = False,\n",
    "                                        crop_masks        = False,\n",
    "                                        score_threshold   = 0.15)\n",
    "        #torch.cuda.synchronize()\n",
    "\n",
    "    with timer.env('Copy'):\n",
    "        if cfg.eval_mask_branch:\n",
    "            # Masks are drawn on the GPU, so don't copy\n",
    "            masks = t[3][:15]\n",
    "        classes, scores, boxes = [x[:15].cpu().numpy() for x in t[:3]]\n",
    "\n",
    "    num_dets_to_consider = min(15, classes.shape[0])\n",
    "    for j in range(num_dets_to_consider):\n",
    "        if scores[j] < 0.15:\n",
    "            num_dets_to_consider = j\n",
    "            break\n",
    "    \n",
    "    if num_dets_to_consider == 0:\n",
    "        # No detections found so just output the original image\n",
    "        return (img_gpu * 255).byte().cpu().numpy()\n",
    "\n",
    "    # Quick and dirty lambda for selecting the color for a particular index\n",
    "    # Also keeps track of a per-gpu color cache for maximum speed\n",
    "    def get_color(j, on_gpu=None):\n",
    "        global color_cache\n",
    "        color_idx = (classes[j] * 5 if class_color else j * 5) % len(COLORS)\n",
    "        \n",
    "        if on_gpu is not None and color_idx in color_cache[on_gpu]:\n",
    "            return color_cache[on_gpu][color_idx]\n",
    "        else:\n",
    "            color = COLORS[color_idx]\n",
    "            if not undo_transform:\n",
    "                # The image might come in as RGB or BRG, depending\n",
    "                color = (color[2], color[1], color[0])\n",
    "            if on_gpu is not None:\n",
    "                color = torch.Tensor(color).to(on_gpu).float() / 255.\n",
    "                color_cache[on_gpu][color_idx] = color\n",
    "            return color\n",
    "\n",
    "    # First, draw the masks on the GPU where we can do it really fast\n",
    "    # Beware: very fast but possibly unintelligible mask-drawing code ahead\n",
    "    # I wish I had access to OpenGL or Vulkan but alas, I guess Pytorch tensor operations will have to suffice\n",
    "    if True:\n",
    "        # After this, mask is of size [num_dets, h, w, 1]\n",
    "        masks = masks[:num_dets_to_consider, :, :, None]\n",
    "        \n",
    "        # Prepare the RGB images for each mask given their color (size [num_dets, h, w, 1])\n",
    "        colors = torch.cat([(torch.Tensor(get_color(j)).float() / 255 ).view(1, 1, 1, 3) for j in range(num_dets_to_consider)], dim=0)\n",
    "        masks_color = masks.repeat(1, 1, 1, 3) * colors * mask_alpha\n",
    "\n",
    "        # This is 1 everywhere except for 1-mask_alpha where the mask is\n",
    "        inv_alph_masks = masks * (-mask_alpha) + 1\n",
    "        \n",
    "        # I did the math for this on pen and paper. This whole block should be equivalent to:\n",
    "        #    for j in range(num_dets_to_consider):\n",
    "        #        img_gpu = img_gpu * inv_alph_masks[j] + masks_color[j]\n",
    "        masks_color_summand = masks_color[0]\n",
    "        if num_dets_to_consider > 1:\n",
    "            inv_alph_cumul = inv_alph_masks[:(num_dets_to_consider-1)].cumprod(dim=0)\n",
    "            masks_color_cumul = masks_color[1:] * inv_alph_cumul\n",
    "            masks_color_summand += masks_color_cumul.sum(dim=0)\n",
    "\n",
    "        img_gpu = img_gpu * inv_alph_masks.prod(dim=0) + masks_color_summand\n",
    "        \n",
    "    # Then draw the stuff that needs to be done on the cpu\n",
    "    # Note, make sure this is a uint8 tensor or opencv will not anti alias text for whatever reason\n",
    "    img_numpy = (img_gpu * 255).byte().cpu().numpy()\n",
    "    \n",
    "    if True:\n",
    "        for j in reversed(range(num_dets_to_consider)):\n",
    "            x1, y1, x2, y2 = boxes[j, :]\n",
    "            color = get_color(j)\n",
    "            score = scores[j]\n",
    "\n",
    "            if True:\n",
    "                cv2.rectangle(img_numpy, (x1, y1), (x2, y2), color, 1)\n",
    "\n",
    "            if True:\n",
    "                _class = cfg.dataset.class_names[classes[j]]\n",
    "                text_str = '%s: %.2f' % (_class, score) if True else _class\n",
    "\n",
    "                font_face = cv2.FONT_HERSHEY_DUPLEX\n",
    "                font_scale = 0.6\n",
    "                font_thickness = 1\n",
    "\n",
    "                text_w, text_h = cv2.getTextSize(text_str, font_face, font_scale, font_thickness)[0]\n",
    "\n",
    "                text_pt = (x1, y1 - 3)\n",
    "                text_color = [255, 255, 255]\n",
    "\n",
    "                cv2.rectangle(img_numpy, (x1, y1), (x1 + text_w, y1 - text_h - 4), color, -1)\n",
    "                cv2.putText(img_numpy, text_str, text_pt, font_face, font_scale, text_color, font_thickness, cv2.LINE_AA)\n",
    "    \n",
    "    return img_numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Config' object has no attribute 'mask_proto_debug'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-612eb1c4589e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m                 'proto': torch.from_numpy(pred_onx[4])}, None)\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mimg_numpy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprep_display\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mundo_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_numpy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-068c5b4c6875>\u001b[0m in \u001b[0;36mprep_display\u001b[0;34m(dets_out, img, h, w, undo_transform, class_color, mask_alpha)\u001b[0m\n\u001b[1;32m     13\u001b[0m         t = postprocess(dets_out, w, h, visualize_lincomb = False,\n\u001b[1;32m     14\u001b[0m                                         \u001b[0mcrop_masks\u001b[0m        \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                                         score_threshold   = 0.15)\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;31m#torch.cuda.synchronize()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/ernestas/D1/workdir/yolact/layers/output_utils.py\u001b[0m in \u001b[0;36mpostprocess\u001b[0;34m(det_output, w, h, batch_idx, interpolation_mode, visualize_lincomb, crop_masks, score_threshold)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;31m# Test flag, do not upvote\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask_proto_debug\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m             \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'scripts/proto.npy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Config' object has no attribute 'mask_proto_debug'"
     ]
    }
   ],
   "source": [
    "from layers import Detect\n",
    "\n",
    "\n",
    "detect = Detect(81, bkg_label=0, top_k=200, conf_thresh=0.05, nms_thresh=0.5)\n",
    "preds = detect({'loc': torch.from_numpy(pred_onx[0]), \n",
    "                'conf': torch.from_numpy(pred_onx[1]), \n",
    "                'mask': torch.from_numpy(pred_onx[2]), \n",
    "                'priors': torch.from_numpy(pred_onx[3]), \n",
    "                'proto': torch.from_numpy(pred_onx[4])}, None)\n",
    "\n",
    "img_numpy = prep_display(preds, frame, None, None, undo_transform=False)\n",
    "plt.imshow(img_numpy)\n",
    "plt.title(path)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
